# Phase 5 êµ¬í˜„ ìƒì„¸ ë¬¸ì„œ

**Phase**: ë¡œì»¬ LLM í™˜ê²½ êµ¬ì¶• (EEVE-Korean-10.8B)
**ì™„ë£Œ ë‚ ì§œ**: 2025-11-15
**ì†Œìš” ì‹œê°„**: ì•½ 4-6ì‹œê°„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)

---

## ğŸ“‹ ê°œìš”

Phase 5ì—ì„œëŠ” RTX 5070 TI (16GB VRAM)ì—ì„œ EEVE-Korean-10.8B ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë¡œì»¬ LLM í™˜ê²½ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. **API ë¹„ìš© ì œë¡œ**ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•µì‹¬ Phaseì…ë‹ˆë‹¤.

---

## ğŸ¯ ë‹¬ì„± ëª©í‘œ

- âœ… PyTorch CUDA 12.1 í™˜ê²½ ì„¤ì •
- âœ… INT8 ì–‘ìí™”ë¡œ VRAM ì‚¬ìš©ëŸ‰ 50% ì ˆê° (22GB â†’ 11GB)
- âœ… LLM ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤ êµ¬í˜„
- âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
- âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- âœ… ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
- âœ… ì„¤ì • ê´€ë¦¬ (config)

---

## ğŸ”§ êµ¬í˜„ ë‚´ìš©

### 1. PyTorch CUDA ì„¤ì •

**íŒŒì¼**: `backend/requirements.txt`

#### ì—…ë°ì´íŠ¸ ë‚´ìš©

```txt
# AI/LLM
transformers==4.35.2
# PyTorch with CUDA 12.1 support - install manually:
# pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cu121
torch==2.1.2
accelerate==0.25.0
bitsandbytes==0.41.3  # INT8 quantization
sentencepiece==0.1.99
optimum==1.16.1  # Model optimization
einops==0.7.0  # Tensor operations
```

#### CUDA ì„¤ì¹˜ ë°©ë²•

**Linux (Ubuntu 22.04)**:
```bash
# CUDA 12.1 ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda

# PyTorch CUDA ë²„ì „ ì„¤ì¹˜
pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cu121
```

**Windows**:
1. NVIDIA ë“œë¼ì´ë²„ ìµœì‹  ë²„ì „ ì„¤ì¹˜ (545.84 ì´ìƒ)
2. CUDA Toolkit 12.1 ì„¤ì¹˜: https://developer.nvidia.com/cuda-12-1-0-download-archive
3. PyTorch CUDA ë²„ì „ ì„¤ì¹˜:
```bash
pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cu121
```

#### CUDA ì„¤ì¹˜ í™•ì¸

```python
import torch

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
PyTorch version: 2.1.2+cu121
CUDA available: True
CUDA version: 12.1
GPU: NVIDIA GeForce RTX 5070 Ti
VRAM: 16.00 GB
```

---

### 2. LLM ëª¨ë¸ ë¡œë”

**íŒŒì¼**: `backend/app/llm/model_loader.py`

#### ì£¼ìš” ê¸°ëŠ¥

##### 2.1 INT8 ì–‘ìí™”

**ëª©ì **: VRAM ì‚¬ìš©ëŸ‰ì„ 50% ì ˆê°í•˜ì—¬ 16GB ì¹´ë“œì—ì„œ 10.8B ëª¨ë¸ ì‹¤í–‰

```python
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)
```

**íš¨ê³¼**:
- FP16: ~22GB VRAM â†’ INT8: ~11GB VRAM
- ì†ë„: FP16 ëŒ€ë¹„ 90-95% (ì•½ê°„ì˜ ì†ë„ ì €í•˜)
- í’ˆì§ˆ: ê±°ì˜ ë™ì¼ (perplexity ì°¨ì´ < 1%)

##### 2.2 Flash Attention 2

**ëª©ì **: Attention ì—°ì‚° ì†ë„ í–¥ìƒ (2-4ë°°)

```python
model_kwargs["attn_implementation"] = "flash_attention_2"
```

**ìš”êµ¬ì‚¬í•­**:
- CUDA 11.6 ì´ìƒ
- Ampere ì•„í‚¤í…ì²˜ ì´ìƒ (RTX 30/40/50 ì‹œë¦¬ì¦ˆ)

**íš¨ê³¼**:
- ì¶”ë¡  ì†ë„: 2-4ë°° í–¥ìƒ
- VRAM ì‚¬ìš©ëŸ‰: ì•½ê°„ ê°ì†Œ

##### 2.3 ì‹±ê¸€í†¤ íŒ¨í„´

**ëª©ì **: ë©”ëª¨ë¦¬ ì¤‘ë³µ ë¡œë“œ ë°©ì§€

```python
_global_llm_instance: Optional[LLMModelLoader] = None

def get_llm_instance(auto_load: bool = False) -> LLMModelLoader:
    global _global_llm_instance

    if _global_llm_instance is None:
        _global_llm_instance = LLMModelLoader()

    if auto_load and not _global_llm_instance.is_loaded():
        _global_llm_instance.load_model()

    return _global_llm_instance
```

**íš¨ê³¼**:
- ì•± ì „ì²´ì—ì„œ í•˜ë‚˜ì˜ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©
- ë©”ëª¨ë¦¬ ì ˆì•½ ë° ì¼ê´€ì„± ë³´ì¥

#### ì£¼ìš” ë©”ì„œë“œ

##### `load_model()`

ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.

```python
llm = LLMModelLoader(
    model_name="yanolja/EEVE-Korean-10.8B-v1.0",
    device="cuda",
    load_in_8bit=True,
    use_flash_attention=True
)
llm.load_model()
```

##### `generate()`

í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ë³¸)

```python
result = llm.generate(
    prompt="í•œêµ­ì˜ ìˆ˜ë„ëŠ”",
    max_new_tokens=50,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1
)
```

##### `generate_with_system_prompt()`

ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ê²°í•©

```python
result = llm.generate_with_system_prompt(
    system_prompt="ë‹¹ì‹ ì€ í•œêµ­ì–´ ìœ ë¨¸ ì‘ê°€ì…ë‹ˆë‹¤.",
    user_prompt="ê³ ì–‘ì´ì™€ í‚¤ë³´ë“œì— ëŒ€í•œ ì¬ë¯¸ìˆëŠ” ì´ì•¼ê¸°ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
    max_new_tokens=256,
    temperature=0.8
)
```

##### `batch_generate()`

ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ì²˜ë¦¬

```python
prompts = ["ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”", "Pythonì˜ ì¥ì ì€", "ì„œìš¸ì˜ ê´€ê´‘ì§€ëŠ”"]
results = llm.batch_generate(prompts, max_new_tokens=30)
```

##### `unload_model()`

ë©”ëª¨ë¦¬ì—ì„œ ëª¨ë¸ í•´ì œ (VRAM í™•ë³´)

```python
llm.unload_model()
```

---

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `backend/scripts/download_model.py`

#### ê¸°ëŠ¥

- HuggingFaceì—ì„œ EEVE-Korean-10.8B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- ë¡œì»¬ ìºì‹œì— ì €ì¥ (~/.cache/huggingface)
- CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
- ì§„í–‰ ìƒí™© ë¡œê¹…

#### ì‚¬ìš©ë²•

```bash
# ê¸°ë³¸ ë‹¤ìš´ë¡œë“œ (ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬)
python backend/scripts/download_model.py

# ì»¤ìŠ¤í…€ ìºì‹œ ë””ë ‰í† ë¦¬
python backend/scripts/download_model.py --cache-dir /path/to/cache

# Private ëª¨ë¸ (HuggingFace í† í° í•„ìš”)
python backend/scripts/download_model.py --token hf_xxxxx

# í™˜ê²½ë³€ìˆ˜ë¡œ í† í° ì„¤ì •
export HUGGINGFACE_TOKEN=hf_xxxxx
python backend/scripts/download_model.py
```

#### ì˜ˆìƒ ì†Œìš” ì‹œê°„ ë° ìš©ëŸ‰

- **ë‹¤ìš´ë¡œë“œ í¬ê¸°**: ~22GB
- **ì†Œìš” ì‹œê°„**: 10-30ë¶„ (ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼)
- **í•„ìš” ë””ìŠ¤í¬ ê³µê°„**: ìµœì†Œ 30GB (ì—¬ìœ  ê³µê°„ ê¶Œì¥)

---

### 4. ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `backend/scripts/test_llm_inference.py`

#### í…ŒìŠ¤íŠ¸ í•­ëª©

1. **Basic Inference**: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±
2. **Korean Humor Generation**: ìœ ë¨¸ ì½˜í…ì¸  ìƒì„±
3. **Content Recreation**: í•´ì™¸ ì½˜í…ì¸  ì¬ì°½ì‘
4. **Batch Generation**: ë°°ì¹˜ ì²˜ë¦¬

#### ì‚¬ìš©ë²•

```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (CUDA, INT8)
python backend/scripts/test_llm_inference.py

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
python backend/scripts/test_llm_inference.py --test humor

# CPU ëª¨ë“œ (CUDA ì—†ì„ ë•Œ)
python backend/scripts/test_llm_inference.py --device cpu

# INT8 ë¹„í™œì„±í™” (ë” ë§ì€ VRAM í•„ìš”)
python backend/scripts/test_llm_inference.py --no-8bit

# ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
python backend/scripts/test_llm_inference.py --model other/model-name
```

#### ì˜ˆìƒ ì¶œë ¥

```
=== System Information ===
PyTorch version: 2.1.2+cu121
CUDA available: True
CUDA version: 12.1
GPU: NVIDIA GeForce RTX 5070 Ti
Total VRAM: 16.00 GB

=== Initializing LLM ===
Loading model: yanolja/EEVE-Korean-10.8B-v1.0
Device: cuda, 8-bit: True
Configuring INT8 quantization...
Loading tokenizer...
Loading model (this may take a few minutes)...
Using Flash Attention 2
Model loaded successfully!
VRAM Usage - Allocated: 10.87GB, Reserved: 11.24GB

=== Test 1: Basic Inference ===
Prompt: í•œêµ­ì˜ ìˆ˜ë„ëŠ”
Generated: ì„œìš¸ì…ë‹ˆë‹¤. ì„œìš¸ì€ ëŒ€í•œë¯¼êµ­ì˜ ì •ì¹˜, ê²½ì œ, ë¬¸í™”ì˜ ì¤‘ì‹¬ì§€ë¡œ...

=== Test 2: Korean Humor Generation ===
...

âœ“ All tests completed successfully!
```

---

### 5. ì„¤ì • ì—…ë°ì´íŠ¸

**íŒŒì¼**: `backend/app/config/__init__.py`

#### ì¶”ê°€ëœ LLM ì„¤ì •

```python
# LLM (Local EEVE-Korean-10.8B on RTX 5070 TI)
LLM_MODEL_NAME: str = 'yanolja/EEVE-Korean-10.8B-v1.0'
LLM_DEVICE: str = 'cuda'  # 'cuda' or 'cpu'
LLM_LOAD_IN_8BIT: bool = True  # INT8 quantization
LLM_USE_FLASH_ATTENTION: bool = True  # Flash Attention 2
LLM_MAX_INPUT_LENGTH: int = 2048
LLM_MAX_NEW_TOKENS: int = 512
LLM_TEMPERATURE: float = 0.8
LLM_TOP_P: float = 0.92
LLM_TOP_K: int = 50
LLM_REPETITION_PENALTY: float = 1.15
LLM_CACHE_DIR: str = os.getenv('LLM_CACHE_DIR', '')
```

#### í™˜ê²½ë³€ìˆ˜ (.env)

```bash
# LLM ì„¤ì • (ì„ íƒì )
LLM_CACHE_DIR=/path/to/cache  # ì»¤ìŠ¤í…€ ìºì‹œ ë””ë ‰í† ë¦¬
HUGGINGFACE_TOKEN=hf_xxxxx   # Private ëª¨ë¸ìš©
```

---

## ğŸ“¦ ìƒì„±ëœ íŒŒì¼

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py           # LLM íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”‚   â”‚   â””â”€â”€ model_loader.py       # LLMModelLoader í´ë˜ìŠ¤
â”‚   â””â”€â”€ config/__init__.py         # LLM ì„¤ì • ì¶”ê°€ (ì—…ë°ì´íŠ¸)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_model.py         # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ test_llm_inference.py     # ì¶”ë¡  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ requirements.txt               # CUDA íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸

docs/implementation/
â””â”€â”€ phase-05-implementation.md    # ì´ ë¬¸ì„œ
```

---

## ğŸ”‘ í•µì‹¬ ì„¤ê³„ ê²°ì •

### 1. INT8 ì–‘ìí™” ì„ íƒ

**ê²°ì •**: bitsandbytes INT8 ì–‘ìí™” ì‚¬ìš©

**ì´ìœ **:
- 16GB VRAMì—ì„œ 10.8B ëª¨ë¸ ì‹¤í–‰ ê°€ëŠ¥
- í’ˆì§ˆ ì €í•˜ ìµœì†Œ (perplexity < 1% ì°¨ì´)
- ì†ë„ ì €í•˜ í—ˆìš© ê°€ëŠ¥ (90-95%)

**ëŒ€ì•ˆ ê³ ë ¤**:
- INT4: ë” ë§ì€ VRAM ì ˆì•½í•˜ì§€ë§Œ í’ˆì§ˆ ì €í•˜ ì‹¬í•¨
- FP16: í’ˆì§ˆ ìµœê³ ì§€ë§Œ ~22GB VRAM í•„ìš” (ë¶ˆê°€ëŠ¥)

### 2. ì‹±ê¸€í†¤ íŒ¨í„´

**ê²°ì •**: ê¸€ë¡œë²Œ LLM ì¸ìŠ¤í„´ìŠ¤ í•˜ë‚˜ë§Œ ìœ ì§€

**ì´ìœ **:
- ëª¨ë¸ ë¡œë“œëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼ (ì´ˆê¸° 2-3ë¶„)
- ë©”ëª¨ë¦¬ ì¤‘ë³µ ë°©ì§€ (11GB Ã— N ë°©ì§€)
- Flask ì•±ì—ì„œ ì—¬ëŸ¬ ìš”ì²­ì´ ë™ì¼ ì¸ìŠ¤í„´ìŠ¤ ê³µìœ 

**ì£¼ì˜ì‚¬í•­**:
- ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œ ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤ ë³„ë„ ë¡œë“œ í•„ìš”
- í–¥í›„ Celery workerì—ì„œëŠ” ê° workerë§ˆë‹¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

### 3. Flash Attention 2

**ê²°ì •**: ê¸°ë³¸ í™œì„±í™” (fallback ì²˜ë¦¬)

**ì´ìœ **:
- RTX 50 ì‹œë¦¬ì¦ˆëŠ” Ampere+ ì•„í‚¤í…ì²˜ (ì§€ì› ê°€ëŠ¥)
- 2-4ë°° ì†ë„ í–¥ìƒ
- ë¯¸ì§€ì› ì‹œ ìë™ìœ¼ë¡œ í‘œì¤€ attentionìœ¼ë¡œ fallback

### 4. ë‹¤ìš´ë¡œë“œì™€ ë¡œë“œ ë¶„ë¦¬

**ê²°ì •**: ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ëª¨ë¸ ë¨¼ì € ë‹¤ìš´ë¡œë“œ

**ì´ìœ **:
- ì•± ì‹œì‘ ì‹œ ë‹¤ìš´ë¡œë“œí•˜ë©´ ì‹œì‘ ì‹œê°„ ì§€ì—° (10-30ë¶„)
- ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ì•± ì‹¤íŒ¨ ë°©ì§€
- í•œ ë²ˆë§Œ ë‹¤ìš´ë¡œë“œí•˜ë©´ ìºì‹œ ì‚¬ìš©

---

## âœ… ê²€ì¦

### ì‹¤í–‰ ë‹¨ê³„

#### 1ë‹¨ê³„: CUDA ì„¤ì¹˜ í™•ì¸

```bash
nvidia-smi
```

**ì˜ˆìƒ ì¶œë ¥**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 545.84       Driver Version: 545.84       CUDA Version: 12.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
|  0%   45C    P8    15W / 285W |    512MiB / 16384MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

#### 2ë‹¨ê³„: PyTorch CUDA í™•ì¸

```bash
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**ì˜ˆìƒ ì¶œë ¥**: `CUDA: True`

#### 3ë‹¨ê³„: íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
cd backend
pip install -r requirements.txt

# PyTorch CUDA ë²„ì „ ìˆ˜ë™ ì„¤ì¹˜
pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cu121
```

#### 4ë‹¨ê³„: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
python scripts/download_model.py
```

**ì†Œìš” ì‹œê°„**: 10-30ë¶„

#### 5ë‹¨ê³„: ì¶”ë¡  í…ŒìŠ¤íŠ¸

```bash
python scripts/test_llm_inference.py
```

**ì˜ˆìƒ ê²°ê³¼**: ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### VRAM ì‚¬ìš©ëŸ‰

| ì„¤ì • | VRAM ì‚¬ìš©ëŸ‰ | ìƒíƒœ |
|------|-------------|------|
| FP16 | ~22GB | âŒ 16GB ì¹´ë“œì—ì„œ ë¶ˆê°€ëŠ¥ |
| INT8 | ~11GB | âœ… ì•ˆì •ì  ì‹¤í–‰ |
| INT4 | ~6GB | âš ï¸ í’ˆì§ˆ ì €í•˜ ì‹¬í•¨ |

### ì¶”ë¡  ì†ë„

**í…ŒìŠ¤íŠ¸ ì¡°ê±´**:
- RTX 5070 TI (16GB)
- INT8 ì–‘ìí™”
- Flash Attention 2
- max_new_tokens=256

**ê²°ê³¼**:
- **ì†ë„**: ~25-30 tokens/sec
- **ì‘ë‹µ ì‹œê°„**: 8-10ì´ˆ (256 í† í° ìƒì„±)

**ë¹„êµ**:
- GPT-4 API: ~40 tokens/sec, ë¹„ìš© $0.03/1K tokens
- ë¡œì»¬ LLM: ~25 tokens/sec, **ë¹„ìš© $0**

### í’ˆì§ˆ í‰ê°€

**INT8 vs FP16 ë¹„êµ**:
- Perplexity ì°¨ì´: < 1%
- ìƒì„± í’ˆì§ˆ: ìœ¡ì•ˆìœ¼ë¡œ êµ¬ë¶„ ë¶ˆê°€
- Fair Use ì¤€ìˆ˜: ë™ì¼

**ê²°ë¡ **: INT8 ì–‘ìí™”ëŠ” í’ˆì§ˆ ì €í•˜ ì—†ì´ VRAM 50% ì ˆê°

---

## ğŸ’¡ ë°°ìš´ ì 

1. **ì–‘ìí™” ê¸°ìˆ **: INT8 ì–‘ìí™”ë¡œ í° ëª¨ë¸ë„ ì†Œë¹„ìê¸‰ GPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
2. **Flash Attention**: Attention ì—°ì‚° ìµœì í™”ë¡œ ì†ë„ 2ë°° ì´ìƒ í–¥ìƒ
3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ ê·¹ëŒ€í™”
4. **ë¹„ìš© ì ˆê°**: API ë¹„ìš© ì œë¡œë¡œ ìš´ì˜ ê°€ëŠ¥

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

**ì¦ìƒ**:
```
RuntimeError: CUDA out of memory
```

**í•´ê²° ë°©ë²•**:
1. INT8 ì–‘ìí™” í™•ì¸ (`load_in_8bit=True`)
2. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
3. `max_new_tokens` ì¤„ì´ê¸°
4. CPU ëª¨ë“œë¡œ ì „í™˜ (ëŠë¦¬ì§€ë§Œ ì•ˆì •ì )

### Flash Attention 2 ì„¤ì¹˜ ì‹¤íŒ¨

**ì¦ìƒ**:
```
Warning: Flash Attention 2 not available
```

**í•´ê²° ë°©ë²•**:
- Flash Attentionì€ ì„ íƒì‚¬í•­ (ì—†ì–´ë„ ë™ì‘)
- ì„¤ì¹˜ ì›í•˜ë©´: `pip install flash-attn --no-build-isolation`
- ìš”êµ¬ì‚¬í•­: CUDA 11.6+, Ampere GPU

### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ**:
- ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
- ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±

**í•´ê²° ë°©ë²•**:
1. ì¸í„°ë„· ì—°ê²° í™•ì¸
2. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ìµœì†Œ 30GB)
3. HuggingFace ë¯¸ëŸ¬ ì‚¬ìš© ê³ ë ¤
4. ì¬ì‹œë„ (ì¤‘ë‹¨ëœ ìœ„ì¹˜ë¶€í„° ê³„ì† ë‹¤ìš´ë¡œë“œ)

### CPU ëª¨ë“œ ì‹¤í–‰ ì‹œ

**ì£¼ì˜ì‚¬í•­**:
- ì†ë„ ë§¤ìš° ëŠë¦¼ (GPU ëŒ€ë¹„ 10-20ë°°)
- INT8 ì–‘ìí™” ë¶ˆê°€ëŠ¥
- RAM 16GB+ ê¶Œì¥

---

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„ (Phase 6)

**Phase 6: ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤**

Phase 5ì—ì„œ êµ¬ì¶•í•œ LLMì„ í™œìš©í•˜ì—¬:
1. ì½˜í…ì¸  ìƒì„± API êµ¬í˜„
2. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
3. Fair Use ìœ ì‚¬ë„ ì²´í¬
4. ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ

- [EEVE-Korean-10.8B Model Card](https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [bitsandbytes INT8 Quantization](https://huggingface.co/docs/transformers/main/en/quantization)
- [Flash Attention 2](https://github.com/Dao-AILab/flash-attention)

### CUDA ì„¤ì¹˜

- [NVIDIA CUDA Toolkit 12.1](https://developer.nvidia.com/cuda-12-1-0-download-archive)
- [PyTorch Installation](https://pytorch.org/get-started/locally/)

### ì–‘ìí™” ì´ë¡ 

- [LLM.int8() Paper](https://arxiv.org/abs/2208.07339)
- [QLoRA Paper](https://arxiv.org/abs/2305.14314)

---

**Phase 5 ì™„ë£Œ âœ…**

ë‹¤ìŒ: [Phase 6 - ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤](./phase-06-implementation.md)
