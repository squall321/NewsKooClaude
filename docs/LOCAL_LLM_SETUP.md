# ë¡œì»¬ LLM ì„¤ì • ê°€ì´ë“œ

> RTX 5070 TI (16GB VRAM)ë¡œ í•œêµ­ì–´ LLMì„ ì‹¤í–‰í•˜ì—¬ **API ë¹„ìš© ì œë¡œ** ë‹¬ì„±í•˜ê¸°

## ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
3. [CUDA í™˜ê²½ ì„¤ì •](#cuda-í™˜ê²½-ì„¤ì •)
4. [ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ](#ëª¨ë¸-ì„ íƒ-ê°€ì´ë“œ)
5. [ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜](#ëª¨ë¸-ë‹¤ìš´ë¡œë“œ-ë°-ì„¤ì¹˜)
6. [vLLM ì„¤ì •](#vllm-ì„¤ì •)
7. [Flask í†µí•©](#flask-í†µí•©)
8. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
9. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

---

## ê°œìš”

### ì™œ ë¡œì»¬ LLMì¸ê°€?

**ë¹„ìš© ì ˆê°**:
- OpenAI GPT-4: $0.03/1K tokens (ì¶œë ¥ ê¸°ì¤€)
- ì›” 10,000ê°œ ì¬êµ¬ì„± ì‹œ: **~$300/ì›”**
- ë¡œì»¬ LLM: **$0** (ì „ê¸° ë¹„ìš© ~$5/ì›”)

**ì¥ì **:
- âœ… API ë¹„ìš© ì œë¡œ
- âœ… ì†ë„ ì œì–´ ê°€ëŠ¥ (ë°°ì¹˜ ì²˜ë¦¬)
- âœ… ë°ì´í„° í”„ë¼ì´ë²„ì‹œ (ì™¸ë¶€ ì „ì†¡ ì—†ìŒ)
- âœ… ì˜¤í”„ë¼ì¸ ì‘ë™ ê°€ëŠ¥

**ë‹¨ì **:
- âš ï¸ ì´ˆê¸° ì„¤ì • ë³µì¡
- âš ï¸ VRAM ìš”êµ¬ì‚¬í•­
- âš ï¸ í’ˆì§ˆì´ GPT-4ë³´ë‹¤ ë‚®ì„ ìˆ˜ ìˆìŒ (í•˜ì§€ë§Œ ì¶©ë¶„íˆ ì¢‹ìŒ)

---

## ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´ (ìµœì†Œ)
- **GPU**: NVIDIA RTX 5070 TI (16GB VRAM) âœ…
- **RAM**: 16GB ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
- **ì €ì¥ê³µê°„**: 30GB (ëª¨ë¸ ì €ì¥ìš©)
- **CPU**: ë©€í‹°ì½”ì–´ ê¶Œì¥

### í•˜ë“œì›¨ì–´ (ê¶Œì¥)
- **GPU**: RTX 5070 TI / RTX 4090
- **RAM**: 32GB+
- **ì €ì¥ê³µê°„**: 100GB (ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í—˜)
- **CPU**: AMD Ryzen 7+ / Intel i7+

### ì†Œí”„íŠ¸ì›¨ì–´
- **OS**: Ubuntu 22.04 / Windows 11 (WSL2)
- **Python**: 3.10 or 3.11
- **CUDA**: 12.1+
- **cuDNN**: 8.9+
- **PyTorch**: 2.1+

---

## CUDA í™˜ê²½ ì„¤ì •

### 1. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (Linux)

```bash
# í˜„ì¬ ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# ë“œë¼ì´ë²„ê°€ ì—†ë‹¤ë©´ ì„¤ì¹˜
sudo apt update
sudo apt install nvidia-driver-535  # ë²„ì „ì€ ìƒí™©ì— ë§ê²Œ
sudo reboot

# ì„¤ì¹˜ í™•ì¸
nvidia-smi
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.86.10    Driver Version: 535.86.10    CUDA Version: 12.2   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
| 30%   35C    P8    15W / 285W |    512MiB / 16384MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
```

### 2. CUDA Toolkit ì„¤ì¹˜

```bash
# CUDA 12.1 ì„¤ì¹˜ (Ubuntu)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cuda

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda-12.1/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# ì„¤ì¹˜ í™•ì¸
nvcc --version
```

### 3. PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)

```bash
cd backend
python -m venv venv
source venv/bin/activate

# PyTorch 2.1 + CUDA 12.1
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121

# ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0)}')"
```

**ì˜ˆìƒ ì¶œë ¥**:
```
CUDA available: True
Device: NVIDIA GeForce RTX 5070 Ti
```

---

## ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ

### ì¶”ì²œ í•œêµ­ì–´ LLM ëª¨ë¸

| ëª¨ë¸ | í¬ê¸° | VRAM (FP16) | VRAM (INT8) | ì†ë„ | í’ˆì§ˆ | ì¶”ì²œë„ |
|------|------|-------------|-------------|------|------|--------|
| **EEVE-Korean-10.8B** | 10.8B | 22GB | **12GB** | ì¤‘ | â­â­â­â­â­ | ğŸ¥‡ ìµœê³  |
| Llama-3-Open-Ko-8B | 8B | 16GB | **9GB** | ë¹ ë¦„ | â­â­â­â­ | ğŸ¥ˆ ëŒ€ì²´ì•ˆ |
| Mistral-7B-Korean | 7B | 14GB | **8GB** | ë¹ ë¦„ | â­â­â­ | ğŸ¥‰ ê²½ëŸ‰ |
| Polyglot-Ko-12.8B | 12.8B | 26GB | 14GB | ëŠë¦¼ | â­â­â­â­ | âŒ VRAM ì´ˆê³¼ |

**ê²°ë¡ **: **EEVE-Korean-10.8B (INT8 ì–‘ìí™”)** ì‚¬ìš© ê¶Œì¥
- RTX 5070 TI 16GBì— ì™„ë²½íˆ ë§ìŒ
- í•œêµ­ì–´ í’ˆì§ˆ ìµœê³ 
- ìœ ë¨¸ ì¬êµ¬ì„±ì— ì í•©

### EEVE-Korean-10.8B íŠ¹ì§•
- **ê°œë°œì**: ì•¼ë†€ì (Yanolja)
- **ê¸°ë°˜ ëª¨ë¸**: Llama-3
- **íŠ¹í™”**: í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ìƒì„±
- **ë¼ì´ì„ ìŠ¤**: Llama 3 Community License (ìƒì—…ì  ì´ìš© ê°€ëŠ¥)
- **HuggingFace**: `yanolja/EEVE-Korean-10.8B-v1.0`

---

## ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜

### 1. HuggingFace CLI ì„¤ì¹˜

```bash
cd backend
source venv/bin/activate

pip install huggingface-hub[cli]
pip install transformers accelerate bitsandbytes
```

### 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

**ì˜µì…˜ A: ìë™ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)**

```bash
# backend/scripts/download_model.py ìƒì„±
mkdir -p scripts
cat > scripts/download_model.py << 'EOF'
from huggingface_hub import snapshot_download
import os

MODEL_ID = "yanolja/EEVE-Korean-10.8B-v1.0"
MODEL_DIR = "./models/EEVE-Korean-10.8B"

print(f"Downloading {MODEL_ID}...")
print(f"This will take ~20 minutes (14GB download)")

os.makedirs(MODEL_DIR, exist_ok=True)

snapshot_download(
    repo_id=MODEL_ID,
    local_dir=MODEL_DIR,
    local_dir_use_symlinks=False,
    resume_download=True
)

print(f"âœ… Model downloaded to {MODEL_DIR}")
EOF

# ì‹¤í–‰
python scripts/download_model.py
```

**ì˜µì…˜ B: CLIë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ**

```bash
huggingface-cli download yanolja/EEVE-Korean-10.8B-v1.0 \
  --local-dir ./models/EEVE-Korean-10.8B \
  --local-dir-use-symlinks False
```

### 3. ëª¨ë¸ ì–‘ìí™” (INT8)

**VRAMì„ 12GBë¡œ ì¤„ì´ê¸°**

```bash
# backend/scripts/quantize_model.py ìƒì„±
cat > scripts/quantize_model.py << 'EOF'
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import os

MODEL_PATH = "./models/EEVE-Korean-10.8B"
QUANTIZED_PATH = "./models/EEVE-Korean-10.8B-INT8"

print("ğŸ”§ Quantizing model to INT8...")

# INT8 ì–‘ìí™” ì„¤ì •
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False
)

# ëª¨ë¸ ë¡œë“œ (INT8)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16
)

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

# ì €ì¥
os.makedirs(QUANTIZED_PATH, exist_ok=True)
model.save_pretrained(QUANTIZED_PATH)
tokenizer.save_pretrained(QUANTIZED_PATH)

print(f"âœ… Quantized model saved to {QUANTIZED_PATH}")
print(f"VRAM usage reduced: 22GB â†’ 12GB")
EOF

# ì‹¤í–‰ (ì•½ 10ë¶„ ì†Œìš”)
python scripts/quantize_model.py
```

---

## vLLM ì„¤ì •

### vLLMì´ë€?
- **ë¹ ë¥¸ ì¶”ë¡  ì—”ì§„**: HuggingFaceë³´ë‹¤ 2-5ë°° ë¹ ë¦„
- **PagedAttention**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬

### ì„¤ì¹˜

```bash
pip install vllm
```

### vLLM ì„œë²„ ì‹œì‘

```bash
# backend/scripts/start_vllm_server.sh ìƒì„±
cat > scripts/start_vllm_server.sh << 'EOF'
#!/bin/bash

MODEL_PATH="./models/EEVE-Korean-10.8B-INT8"
PORT=8000

echo "ğŸš€ Starting vLLM server on port $PORT..."

python -m vllm.entrypoints.openai.api_server \
  --model $MODEL_PATH \
  --port $PORT \
  --dtype float16 \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.85 \
  --tensor-parallel-size 1
EOF

chmod +x scripts/start_vllm_server.sh

# ì„œë²„ ì‹œì‘
./scripts/start_vllm_server.sh
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### vLLM í…ŒìŠ¤íŠ¸

```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./models/EEVE-Korean-10.8B-INT8",
    "prompt": "ë‹¤ìŒ ì˜ì–´ ìœ ë¨¸ë¥¼ í•œêµ­ ìŠ¤íƒ€ì¼ë¡œ ì¬êµ¬ì„±í•´ì¤˜:\nWhy did the programmer quit his job? Because he didn'\''t get arrays.",
    "max_tokens": 200,
    "temperature": 0.7
  }'
```

---

## Flask í†µí•©

### 1. LLM ëª¨ë¸ ë¡œë”

```python
# backend/app/llm/model_loader.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional
import os

class LLMModelLoader:
    _instance: Optional['LLMModelLoader'] = None

    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def load_model(self, model_path: str):
        """ëª¨ë¸ ë¡œë“œ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)"""
        if self.model is not None:
            print("âœ… Model already loaded")
            return

        print(f"ğŸ”„ Loading model from {model_path}...")

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True
        )

        print(f"âœ… Model loaded on {self.device}")
        print(f"ğŸ“Š VRAM usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    def generate(self, prompt: str, max_length: int = 512, temperature: float = 0.7):
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text
```

### 2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
# backend/app/llm/prompts.py

REWRITE_HUMOR_PROMPT = """ë‹¹ì‹ ì€ ì „ë¬¸ ìœ ë¨¸ ì‘ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í•´ì™¸ ìœ ë¨¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ íŒŒì•…í•˜ê³ , í•œêµ­ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì¬êµ¬ì„±í•´ì£¼ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™**:
1. ì›ë³¸ì„ ë‹¨ìˆœ ë²ˆì—­í•˜ì§€ ë§ê³ , ì•„ì´ë””ì–´ë¥¼ ì°¨ìš©í•˜ì—¬ ìƒˆë¡­ê²Œ ì°½ì‘
2. í•œêµ­ ë¬¸í™” ë§¥ë½ì— ë§ê²Œ ìˆ˜ì • (ì˜ˆ: ë¯¸êµ­ â†’ í•œêµ­, ë‹¬ëŸ¬ â†’ ì›)
3. ìœ ë¨¸ì˜ í•µì‹¬ í¬ì¸íŠ¸ ìœ ì§€
4. ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì‚¬ìš©
5. ì›ë³¸ê³¼ì˜ ìœ ì‚¬ë„ëŠ” 70% ì´í•˜ë¡œ

**ì›ë³¸ ìœ ë¨¸**:
{original_text}

**ì¬êµ¬ì„± ìš”êµ¬ì‚¬í•­**:
- ìŠ¤íƒ€ì¼: {style}  (ì˜ˆ: ìºì£¼ì–¼, ê²©ì‹, ìœ ë¨¸ëŸ¬ìŠ¤)
- ê¸¸ì´: {length}  (ì˜ˆ: ì§§ê²Œ, ì¤‘ê°„, ê¸¸ê²Œ)

**ì¬êµ¬ì„±ëœ ìœ ë¨¸**:
"""

TITLE_GENERATION_PROMPT = """ë‹¤ìŒ ìœ ë¨¸ ê¸€ì„ ì½ê³ , í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“œëŠ” ë§¤ë ¥ì ì¸ ì œëª©ì„ 3ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.

**ê¸€ ë‚´ìš©**:
{content}

**ì œëª© ìš”êµ¬ì‚¬í•­**:
- 15-30ì ê¸¸ì´
- í˜¸ê¸°ì‹¬ ìœ ë°œ
- SEO í‚¤ì›Œë“œ í¬í•¨
- í´ë¦­ë² ì´íŠ¸ì§€ë§Œ ê³¼í•˜ì§€ ì•Šê²Œ

**ì œëª© (3ê°œ)**:
1.
2.
3.
"""

IMPROVE_PARAGRAPH_PROMPT = """ë‹¤ìŒ ë¬¸ë‹¨ì„ ë” ë‚˜ì€ í’ˆì§ˆë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

**ì›ë³¸**:
{paragraph}

**ê°œì„  ìš”ì²­**:
- {improvement_type}  (ì˜ˆ: ëª…í™•ì„± í–¥ìƒ, ìœ ë¨¸ ì¶”ê°€, í†¤ ë³€ê²½)

**ê°œì„ ëœ ë¬¸ë‹¨**:
"""
```

### 3. AI ì¬êµ¬ì„± ì„œë¹„ìŠ¤

```python
# backend/app/services/ai_rewriter.py
from app.llm.model_loader import LLMModelLoader
from app.llm.prompts import REWRITE_HUMOR_PROMPT
from typing import List

class AIRewriter:
    def __init__(self):
        self.llm = LLMModelLoader.get_instance()

    def rewrite_humor(
        self,
        original_text: str,
        style: str = "ìºì£¼ì–¼",
        length: str = "ì¤‘ê°„",
        num_versions: int = 3
    ) -> List[str]:
        """ìœ ë¨¸ ì¬êµ¬ì„± (ì—¬ëŸ¬ ë²„ì „ ìƒì„±)"""

        results = []

        for i in range(num_versions):
            prompt = REWRITE_HUMOR_PROMPT.format(
                original_text=original_text,
                style=style,
                length=length
            )

            generated = self.llm.generate(
                prompt,
                max_length=1024,
                temperature=0.7 + (i * 0.1)  # ë‹¤ì–‘ì„± ì¦ê°€
            )

            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            rewritten = generated.split("**ì¬êµ¬ì„±ëœ ìœ ë¨¸**:")[-1].strip()
            results.append(rewritten)

        return results

    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard)"""
        set1 = set(text1.split())
        set2 = set(text2.split())

        intersection = len(set1 & set2)
        union = len(set1 | set2)

        return intersection / union if union > 0 else 0.0
```

### 4. API ì—”ë“œí¬ì¸íŠ¸

```python
# backend/app/api/ai_assistant.py
from flask import Blueprint, request, jsonify
from app.services.ai_rewriter import AIRewriter

bp = Blueprint('ai', __name__, url_prefix='/api/v1/ai')
rewriter = AIRewriter()

@bp.route('/rewrite', methods=['POST'])
def rewrite_humor():
    """ìœ ë¨¸ ì¬êµ¬ì„± API"""
    data = request.json

    original = data.get('original_text')
    style = data.get('style', 'ìºì£¼ì–¼')
    length = data.get('length', 'ì¤‘ê°„')
    num_versions = data.get('num_versions', 3)

    if not original:
        return jsonify({'error': 'original_text is required'}), 400

    try:
        results = rewriter.rewrite_humor(
            original_text=original,
            style=style,
            length=length,
            num_versions=num_versions
        )

        # ì›ë³¸ê³¼ì˜ ìœ ì‚¬ë„ ì²´í¬
        similarities = [
            rewriter.calculate_similarity(original, r)
            for r in results
        ]

        return jsonify({
            'success': True,
            'versions': [
                {
                    'text': text,
                    'similarity': sim,
                    'warning': sim > 0.7
                }
                for text, sim in zip(results, similarities)
            ]
        })

    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬
def batch_rewrite(texts: List[str]) -> List[str]:
    prompts = [REWRITE_HUMOR_PROMPT.format(original_text=t) for t in texts]

    # vLLMì˜ ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
    results = llm.batch_generate(prompts, batch_size=4)
    return results
```

### 2. ìºì‹±

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def rewrite_with_cache(original_hash: str, style: str):
    # ë™ì¼í•œ ìš”ì²­ì€ ìºì‹œì—ì„œ ë°˜í™˜
    return rewriter.rewrite_humor(original, style)
```

### 3. GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

```python
import torch

def log_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB")
```

---

## ë¬¸ì œ í•´ê²°

### CUDA Out of Memory

**ì¦ìƒ**:
```
RuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**í•´ê²°ì±…**:
1. INT8 ì–‘ìí™” ì‚¬ìš©
2. `max_length` ì¤„ì´ê¸°
3. Batch size ì¤„ì´ê¸°
4. ëª¨ë¸ ì–¸ë¡œë“œ í›„ ì¬ë¡œë“œ

```python
# ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()
```

### ì¶”ë¡  ì†ë„ ëŠë¦¼

**ì¦ìƒ**: í•œ ë²ˆ ìƒì„±ì— 30ì´ˆ+

**í•´ê²°ì±…**:
1. vLLM ì‚¬ìš© (2-5ë°° ë¹ ë¦„)
2. `max_length` ì¤„ì´ê¸° (512 â†’ 256)
3. `torch.compile()` ì‚¬ìš© (PyTorch 2.0+)

```python
model = torch.compile(model)
```

### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

**ì¦ìƒ**:
```
OSError: Can't load model
```

**í•´ê²°ì±…**:
1. ëª¨ë¸ ê²½ë¡œ í™•ì¸
2. ê¶Œí•œ í™•ì¸ (`chmod -R 755 models/`)
3. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (`df -h`)

---

## ë‹¤ìŒ ë‹¨ê³„

1. âœ… ë¡œì»¬ LLM í™˜ê²½ êµ¬ì¶• ì™„ë£Œ
2. ğŸ“ í”„ë¡¬í”„íŠ¸ íŠœë‹ (Few-shot examples ì¶”ê°€)
3. ğŸ§ª ì¬êµ¬ì„± í’ˆì§ˆ í…ŒìŠ¤íŠ¸
4. ğŸ”„ Flask API í†µí•©
5. ğŸ¨ ê´€ë¦¬ì UI ì—°ë™

**Phase 5 ì™„ë£Œ í›„**: [Phase 6: AI ì¬êµ¬ì„± ì—”ì§„](../DEVELOPMENT_ROADMAP.md#phase-6-ai-ì¬êµ¬ì„±-ì—”ì§„---í”„ë¡¬í”„íŠ¸-ì„¤ê³„)ë¡œ ì´ë™í•˜ì„¸ìš”!

---

**ì°¸ê³  ìë£Œ**:
- [vLLM ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/)
- [EEVE ëª¨ë¸ ì¹´ë“œ](https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
